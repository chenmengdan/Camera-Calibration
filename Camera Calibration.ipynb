{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMP SC 8680 Assignment 2: Camera Calibration and Radial Undistortion\n",
    "## Mengdan Chen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the following tutorial:  \n",
    "  [docs.opencv.org](http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_calib3d/py_calibration/py_calibration.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "learn about distortions in camera, intrinsic and extrinsic parameters of camera etc.\n",
    "<br>learn to find these parameters, undistort images etc.\n",
    "# Introduction\n",
    "Some pinhole cameras introduces a lot of distortion to images. Two major distortions are radial distortion and tangential distortion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tangential Distortion\n",
    "Tangential distortion occurs because image taking lense is not aligned perfectly parallel to the imaging plane. So some areas in image may look nearer than expected. It is represented as below:\n",
    "\n",
    "![image](formulas_image/tangential_factor.png)\n",
    "\n",
    "#### Radial Distortion\n",
    "Similarly, another distortion is the radial distortion.Due to radial distortion, straight lines will appear curved. Its effect is more as we move away from the center of image.\n",
    "\n",
    "![Image](formulas_image/radial_factor.png)\n",
    "\n",
    "we need to find five parameters, known as distortion coefficients given by:\n",
    "\n",
    "![Image](formulas_image/distortion_coefficents.png)\n",
    "\n",
    "In addition to this, we need to find a few more information, like intrinsic and extrinsic parameters of a camera. Intrinsic parameters are specific to a camera. It includes information like focal length ( fx,fy), optical centers ( cx,cy) etc. It is also called camera matrix. It depends on the camera only, so once calculated, it can be stored for future purposes. It is expressed as a 3x3 matrix:\n",
    "\n",
    "![Image](formulas_image/camera_matrix.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Extrinsic parameters corresponds to rotation and translation vectors which translates a coordinates of a 3D point to a coordinate system. Here the presence of w is explained by the use of homography coordinate system (and w=Z).We find some specific points in it ( square corners in chess board). We know its coordinates in real world space and we know its coordinates in image. With these data, the distortion coefficients could be solved. we need atleast 10 test patterns.\n",
    "\n",
    "20 sample images of chess board are given. Consider just one image of a chess board. Important input datas needed for camera calibration is a set of 3D real world points and its corresponding 2D image points. 2D image points are OK which we can easily find from the image. These image points are locations where two black squares touch each other in chess boards\n",
    "<br>What about the 3D points from real world space? Those images are taken from a static camera and chess boards are placed at different locations and orientations. So we need to know (X,Y,Z) values. But for simplicity, we can say chess board was kept stationary at XY plane, (so Z=0 always) and camera was moved accordingly. This consideration helps us to find only X,Y values. In this case, the results we get will be in the scale of size of chess board square.\n",
    "<br>3D points are called object points and 2D image points are called image points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "To find pattern in chess board, we use the function, cv2.findChessboardCorners(). We also need to pass what kind of pattern we are looking, like 8x8 grid, 5x5 grid etc. In this example, we use 11x12 grid.It returns the corner points and retval which will be True if pattern is obtained. These corners will be placed in an order (from left-to-right, top-to-bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import numpy, openCV environment\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "%matplotlib inline\n",
    "\n",
    "# termination criteria\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 50, 0.001)\n",
    "\n",
    "# Define the chess board rows and columns\n",
    "rows = 12\n",
    "cols = 12\n",
    "\n",
    "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "objectPoints = np.zeros((rows * cols, 3), np.float32)\n",
    "objectPoints[:, :2] = np.mgrid[0:rows, 0:cols].T.reshape(-1, 2)\n",
    "objectPoints.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrays to store object points and image points from all the images.\n",
    "objectPointsArray = [] # 3d point in real world space\n",
    "imgPointsArray = [] # 2d points in image plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calib_example\\\\Image1.tif',\n",
       " 'calib_example\\\\Image10.tif',\n",
       " 'calib_example\\\\Image11.tif',\n",
       " 'calib_example\\\\Image12.tif',\n",
       " 'calib_example\\\\Image13.tif',\n",
       " 'calib_example\\\\Image14.tif',\n",
       " 'calib_example\\\\Image15.tif',\n",
       " 'calib_example\\\\Image16.tif',\n",
       " 'calib_example\\\\Image17.tif',\n",
       " 'calib_example\\\\Image18.tif',\n",
       " 'calib_example\\\\Image19.tif',\n",
       " 'calib_example\\\\Image2.tif',\n",
       " 'calib_example\\\\Image20.tif',\n",
       " 'calib_example\\\\Image3.tif',\n",
       " 'calib_example\\\\Image4.tif',\n",
       " 'calib_example\\\\Image5.tif',\n",
       " 'calib_example\\\\Image6.tif',\n",
       " 'calib_example\\\\Image7.tif',\n",
       " 'calib_example\\\\Image8.tif',\n",
       " 'calib_example\\\\Image9.tif']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob('calib_example\\*.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runing the foloowing code returns the corner points and retval if pattern is obtained.\n",
    "<br>Here is an example of the result:\n",
    "![Image](example_points.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the image files\n",
    "for path in glob.glob('calib_example\\*.tif'):\n",
    "    # Load the image and convert it to gray scale\n",
    "    img = cv2.imread(path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find the chess board corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (rows, cols), None)\n",
    "\n",
    "    # Make sure the chess board pattern was found in the image\n",
    "    if ret:\n",
    "        # Refine the corner position\n",
    "        corners = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), criteria)\n",
    "        \n",
    "        # Add the object points and the image points to the arrays\n",
    "        objectPointsArray.append(objectPoints)\n",
    "        imgPointsArray.append(corners)\n",
    "\n",
    "        # Draw the corners on the image\n",
    "        cv2.drawChessboardCorners(img, (rows, cols), corners, ret)\n",
    "    \n",
    "    # Display the image\n",
    "    cv2.imshow('chess board', img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration\n",
    "Now we have our object points and image points we are ready to go for calibration. For that we use the function, ***cv2.calibrateCamera()***. It returns the camera matrix, distortion coefficients, rotation and translation vectors etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate the camera and save the results\n",
    "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objectPointsArray, imgPointsArray, gray.shape[::-1], None, None)\n",
    "np.savez('calib.npz', mtx=mtx, dist=dist, rvecs=rvecs, tvecs=tvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.47543410e-01,  7.14418145e-02, -1.83688275e-04,\n",
       "        -2.74411144e-04,  1.05769974e-01]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dist Coef.\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[658.98431343   0.         302.50047925]\n",
      " [  0.         659.73448089 243.58638325]\n",
      " [  0.           0.           1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#show the camera matrix\n",
    "print(mtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-projection Error\n",
    "Re-projection error gives a good estimation of just how exact is the found parameters. This should be as close to zero as possible. Given the intrinsic, distortion, rotation and translation matrices, we first transform the object point to image point using ***cv2.projectPoints()***. Then we calculate the absolute norm between what we got with our transformation and the corner finding algorithm. To find the average error we calculate the arithmetical mean of the errors calculate for all the calibration images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total error:  0.01804429700829216\n"
     ]
    }
   ],
   "source": [
    "# Print the camera calibration error\n",
    "error = 0\n",
    "\n",
    "for i in range(len(objectPointsArray)):\n",
    "    imgPoints, _ = cv2.projectPoints(objectPointsArray[i], rvecs[i], tvecs[i], mtx, dist)\n",
    "    error += cv2.norm(imgPointsArray[i], imgPoints, cv2.NORM_L2) / len(imgPoints)\n",
    "\n",
    "print(\"Total error: \", error / len(objectPointsArray))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undistortion\n",
    "We have got what we were trying. Now we can take an image and undistort it. OpenCV comes with two methods, we will see both. But before that, we can refine the camera matrix based on a free scaling parameter using ***cv2.getOptimalNewCameraMatrix()***. If the scaling parameter alpha=0, it returns undistorted image with minimum unwanted pixels. So it may even remove some pixels at image corners. If alpha=1, all pixels are retained with some extra black images. It also returns an image ROI which can be used to crop the result.\n",
    "\n",
    "**Take a new image (Image2.tif in this case.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 640, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load one of the test images\n",
    "img = cv2.imread('calib_example\\Image1.tif')\n",
    "h, w = img.shape[:2]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the new camera matrix and undistort the image\n",
    "newCameraMtx, roi = cv2.getOptimalNewCameraMatrix(mtx, dist, (w, h), 1, (w, h))\n",
    "#undistortedImg = cv2.undistort(img, mtx, dist, None, newCameraMtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[601.7623291    0.         300.82953281]\n",
      " [  0.         599.89715576 243.63463233]\n",
      " [  0.           0.           1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(newCameraMtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 14, 621, 452)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two methods to undistort image.\n",
    "#### 1. Using cv2.undistort()\n",
    "\n",
    "This is the shortest path. Just call the function and use ROI obtained above to crop the result.\n",
    "<br>use **cv2.undistort()** to undistort the image. and compare it with the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# undistort\n",
    "dst1 = cv2.undistort(src=img, cameraMatrix=mtx, distCoeffs=dist,newCameraMatrix=newCameraMtx)\n",
    "#cv2.imwrite('calibresult.png', dst1)\n",
    "cv2.imshow('chess board', np.hstack((img, dst1)))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crop the undistorted image, and show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop the undistorted image\n",
    "x, y, w, h = roi\n",
    "dst1_croped = dst1 [y:y + h, x:x + w]\n",
    "#dst2 = dst1 [y:y+h, x:x+w]\n",
    "cv2.imshow('chess board', dst1_croped)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Using remapping\n",
    "This is curved path. First find a mapping function from distorted image to undistorted image. Then use the remap function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# undistort\n",
    "mapx, mapy = cv2.initUndistortRectifyMap(mtx, dist, None, newCameraMtx, (w,h), 5)\n",
    "dst2 = cv2.remap(img, mapx, mapy, cv2.INTER_LINEAR)\n",
    "cv2.imshow('chess board', dst2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crop the undistorted image, and show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop the image\n",
    "x, y, w, h = roi\n",
    "dst2_croped = dst2[y:y+h, x:x+w]\n",
    "cv2.imshow('chess board', dst2_croped)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
